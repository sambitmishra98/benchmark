#!/bin/bash
#SBATCH --job-name="mpi-build"
#SBATCH --nodes=1
#SBATCH --exclusive
#SBATCH --gpu-bind=closest
#SBATCH --use-min-nodes
#SBATCH --time=0-06:00:00
#SBATCH --mem=80G
#SBATCH --output=MPI-build-PVC.out
#SBATCH --no-requeue
#SBATCH --partition=pvc
#SBATCH --ntasks=24
#SBATCH --gres=gpu:pvc:4
#SBATCH --cpus-per-gpu=12

# ------------------------------------------------------------------------------
# Check environment
# ------------------------------------------------------------------------------

    #/sw/local/bin/query_gpu.sh

    #nvidia-smi -L
    clinfo -l
    source ~/.bashrc
    echo -e "\n================================================================="

    export BUILD_NAME="pvc"
    export BUILD_TAG="aces"
    export BUILD_PATH="/scratch/user/u.sm121949/EFFORT_BENCHMARK/benchmark/build"

# ------------------------------------------------------------------------------
# Print SLURM settings
# ------------------------------------------------------------------------------

    numnodes=$SLURM_JOB_NUM_NODES
    mpi_tasks_per_node=$(echo "$SLURM_TASKS_PER_NODE" | sed -e  's/^\([0-9][0-9]*\).*$/\1/')
    np=$[${SLURM_JOB_NUM_NODES}*${mpi_tasks_per_node}]

    echo "Running on master node: `hostname`"
    echo "Time: `date`"
    echo "Current directory: `pwd`"
    echo -e "JobID: $SLURM_JOB_ID\n================================================================="
    echo -e "Tasks=${SLURM_NTASKS},\
            nodes=${SLURM_JOB_NUM_NODES}, \
            mpi_tasks_per_node=${mpi_tasks_per_node} \
            (OMP_NUM_THREADS=$OMP_NUM_THREADS)"

# ------------------------------------------------------------------------------
# Setup paths
# ------------------------------------------------------------------------------

    setup_base
    export_all_versions
    add_installation_to_path gcc      $BUILD_GCC_VER     $PKG_LOCAL

# ------------------------------------------------------------------------------
# Build
# ------------------------------------------------------------------------------

CMD="${BUILD_PATH}/mpi.build ${BUILD_NAME} ${BUILD_TAG} ${BUILD_MPICH_VER} ${BUILD_UCX_VER} ${BUILD_OPENMPI_VER}"
echo -e "\n$CMD\n"
eval $CMD

add_installation_to_path ${BUILD_NAME}-${BUILD_TAG}/mpich     ${BUILD_MPICH_VER}     $PKG_LOCAL
# add_installation_to_path ${BUILD_NAME}-${BUILD_TAG}/ucx     ${BUILD_UCX_VER}     $PKG_LOCAL
# add_installation_to_path ${BUILD_NAME}-${BUILD_TAG}/openmpi ${BUILD_OPENMPI_VER} $PKG_LOCAL

which mpirun
mpichversion

# ------------------------------------------------------------------------------
# Test build with osu_microbenchmarks and other CUDA benchmarks
# ------------------------------------------------------------------------------
# 
# CMD="/scratch/user/u.sm121949/EFFORT_BENCHMARK/benchmark/build/osu_microbenchmarks.build build2 ac037 4.1.6"
# echo -e "\n$CMD\n"
# eval $CMD
# 
# which mpirun
# 
# # RUN OSU MICROBENCHMARKS
# echo -e "\nRunning OSU microbenchmarks\n"
# echo -e "\n=================================================================\n"
# echo -e "\n OSU Bandwidth\n"
# mpirun -np 2 $GIT_LOCAL/build2-ac037/osu-micro-benchmarks/libexec/osu-micro-benchmarks/mpi/pt2pt/osu_bw
# echo -e "\n-----------------------------------------------------------------\n"
# echo -e "\n OSU Bi-Directional Bandwidth\n"
# mpirun -np 2 $GIT_LOCAL/build2-ac037/osu-micro-benchmarks/libexec/osu-micro-benchmarks/mpi/pt2pt/osu_bibw
# echo -e "\n-----------------------------------------------------------------\n"
# 