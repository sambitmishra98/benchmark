#!/bin/bash
#SBATCH --job-name="mpi-build"
#SBATCH --nodes=2
##SBATCH --exclusive
#SBATCH --gpu-bind=closest
#SBATCH --use-min-nodes
#SBATCH --time=0-06:00:00
#SBATCH --mem=80G
#SBATCH --output=OSU-test-A100_N2G1.out
#SBATCH --no-requeue
#SBATCH --partition=pvc
#SBATCH --ntasks-per-node=1
#SBATCH --gres=gpu:pvc:1
#SBATCH --cpus-per-gpu=1

# ------------------------------------------------------------------------------
# Check environment
# ------------------------------------------------------------------------------

    source ~/.bashrc

# ------------------------------------------------------------------------------
# Print SLURM settings
# ------------------------------------------------------------------------------

    numnodes=$SLURM_JOB_NUM_NODES
    mpi_tasks_per_node=$(echo "$SLURM_TASKS_PER_NODE" | sed -e  's/^\([0-9][0-9]*\).*$/\1/')
    np=$[${SLURM_JOB_NUM_NODES}*${mpi_tasks_per_node}]

    echo "Running on master node: `hostname`"
    echo "Time: `date`"
    echo "Current directory: `pwd`"
    echo -e "JobID: $SLURM_JOB_ID\n================================================================="
    echo -e "Tasks=${SLURM_NTASKS},\
            nodes=${SLURM_JOB_NUM_NODES}, \
            mpi_tasks_per_node=${mpi_tasks_per_node} \
            (OMP_NUM_THREADS=$OMP_NUM_THREADS)"

# ------------------------------------------------------------------------------
# Setup paths
# ------------------------------------------------------------------------------

    LATEST_BUILD_PVC
        add_installation_to_path gcc                                        $BUILD_GCC_VER       $PKG_LOCAL
        add_installation_to_path ${BUILD_NAME_MPI}-${BUILD_TAG_MPI}/ucx     ${BUILD_UCX_VER}     ${PKG_LOCAL}
        add_installation_to_path ${BUILD_NAME_MPI}-${BUILD_TAG_MPI}/openmpi ${BUILD_OPENMPI_VER} ${PKG_LOCAL}


# ------------------------------------------------------------------------------
# RUN OSU MICROBENCHMARKS
# ------------------------------------------------------------------------------

for SEND in H
do
    for RECV in H
    do
        echo -e "\n=================================================================\n"
        CMD="mpirun -n 2 $GIT_LOCAL/${BUILD_NAME_MPI}-${BUILD_TAG_MPI}/osu-micro-benchmarks/libexec/osu-micro-benchmarks/mpi/pt2pt/osu_bw $SEND $RECV"
        echo $CMD
        eval $CMD
        echo -e "\n=================================================================\n"
        CMD="mpirun -n 2 $GIT_LOCAL/${BUILD_NAME_MPI}-${BUILD_TAG_MPI}/osu-micro-benchmarks/libexec/osu-micro-benchmarks/mpi/pt2pt/osu_bibw $SEND $RECV"
        echo $CMD
        eval $CMD
    done
done
## Postprocess and get the bandwidths of all of them in a csv file format
# nodes,gpus-per-node,D-or-H,D-or-H,bw,bibw
# Witht he above header, give a terminal command to get the csv file
