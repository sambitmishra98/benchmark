#!/bin/bash
#SBATCH --job-name="build2-script"
#SBATCH --nodes=1
##SBATCH --exclusive
#SBATCH --reservation=benchmarking
#SBATCH --gpu-bind=closest
#SBATCH --use-min-nodes
#SBATCH --time=0-06:00:00
#SBATCH --mem=80G
#SBATCH --output=openmpi-build-test_1node_2gpu.out
#SBATCH --no-requeue
#SBATCH --partition=gpu
#SBATCH --ntasks=2
#SBATCH --gres=gpu:h100:2
#SBATCH --cpus-per-gpu=8
#SBATCH --nodelist=ac036

mkdir -p /scratch/user/u.sm121949/EFFORT_BENCHMARK/benchmark/build2-openmpi-ac037
cd /scratch/user/u.sm121949/EFFORT_BENCHMARK/benchmark/build2-openmpi-ac037

# ------------------------------------------------------------------------------
# Check environment
# ------------------------------------------------------------------------------

/sw/local/bin/query_gpu.sh

nvidia-smi -L ; clinfo -l
source ~/.bashrc
echo -e "\n================================================================="

# ------------------------------------------------------------------------------
# Print SLURM settings
# ------------------------------------------------------------------------------

numnodes=$SLURM_JOB_NUM_NODES
mpi_tasks_per_node=$(echo "$SLURM_TASKS_PER_NODE" | sed -e  's/^\([0-9][0-9]*\).*$/\1/')
np=$[${SLURM_JOB_NUM_NODES}*${mpi_tasks_per_node}]

echo "Running on master node: `hostname`"
echo "Time: `date`"
echo "Current directory: `pwd`"
echo -e "JobID: $SLURM_JOB_ID\n================================================================="
echo -e "Tasks=${SLURM_NTASKS},\
         nodes=${SLURM_JOB_NUM_NODES}, \
         mpi_tasks_per_node=${mpi_tasks_per_node} \
         (OMP_NUM_THREADS=$OMP_NUM_THREADS)"

# ------------------------------------------------------------------------------
# Setup paths
# ------------------------------------------------------------------------------

setup_base
export_all_versions
add_to_path_cuda_12_3_0
add_installation_to_path gcc      $BUILD_GCC_VER     $PKG_LOCAL

export BUILD_UCX_VER=1.15.0
export BUILD_OPENMPI_VER=5.0.1

# ------------------------------------------------------------------------------
# Build
# ------------------------------------------------------------------------------

CMD="/scratch/user/u.sm121949/EFFORT_BENCHMARK/benchmark/build/openmpi.build build2 ac037 1.15.0 4.1.6"
echo -e "\n$CMD\n"
eval $CMD

add_installation_to_path final-compute-amd/ucx     $BUILD_UCX_VER     $PKG_LOCAL
add_installation_to_path final-compute-amd/openmpi $BUILD_OPENMPI_VER $PKG_LOCAL

# ------------------------------------------------------------------------------
# Test build with osu_microbenchmarks and other CUDA benchmarks
# ------------------------------------------------------------------------------

CMD="/scratch/user/u.sm121949/EFFORT_BENCHMARK/benchmark/build/osu_microbenchmarks.build build2 ac037 4.1.6"
echo -e "\n$CMD\n"
eval $CMD

which mpirun

# RUN OSU MICROBENCHMARKS
echo -e "\nRunning OSU microbenchmarks\n"
echo -e "\n=================================================================\n"
echo -e "\n OSU Bandwidth\n"
mpirun -np 2 $GIT_LOCAL/build2-ac037/osu-micro-benchmarks/libexec/osu-micro-benchmarks/mpi/pt2pt/osu_bw
echo -e "\n-----------------------------------------------------------------\n"
echo -e "\n OSU Bi-Directional Bandwidth\n"
mpirun -np 2 $GIT_LOCAL/build2-ac037/osu-micro-benchmarks/libexec/osu-micro-benchmarks/mpi/pt2pt/osu_bibw
echo -e "\n-----------------------------------------------------------------\n"
